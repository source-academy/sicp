<SECTION>
  <NAME>Concurrency: Time Is of the Essence</NAME>

  <SECTIONCONTENT/>

  <LABEL NAME="sec:time-is-of-the-essence"/>

  <INDEX>concurrency<OPEN/></INDEX>

  <TEXT>
    We<APOS/>ve seen the power of computational objects with local state as
    tools for modeling.  Yet, as
    section<SPACE/><REF NAME="sec:costs-of-assignment"/>
    warned, this power extracts a price: the loss of referential
    transparency, giving rise to a thicket of questions about sameness and
    change, and the need to abandon the substitution model of evaluation in
    favor of the more intricate environment model.
  </TEXT>

  <TEXT>
    The central issue lurking beneath the complexity of state, sameness,
    and change is that by introducing assignment we are forced to admit
    <INDEX>time<SUBINDEX>assignment and</SUBINDEX></INDEX>
    <EM>time</EM> into our computational models.  Before we introduced
    assignment, all our programs were timeless, in the sense that any
    expression that has a value always has the same value.  In contrast,
    recall the example of modeling withdrawals from a bank account
    and returning the resulting balance,
    introduced at the beginning of
    section<SPACE/><REF NAME="sec:local-state-variables"/>:

    <SNIPPET>
      <NAME>withdraw_example_3_4</NAME>
      <REQUIRES>withdraw</REQUIRES>
      <EXPECTED>75</EXPECTED>
      <SCHEME>
(withdraw 25)
      </SCHEME>
      <JAVASCRIPT>
withdraw(25);
      </JAVASCRIPT>
      <SCHEMEOUTPUT>
75
      </SCHEMEOUTPUT>
      <JAVASCRIPT_OUTPUT>
75
      </JAVASCRIPT_OUTPUT>
    </SNIPPET>

    <SNIPPET>
      <NAME>withdraw_example_3_4_second</NAME>
      <REQUIRES>withdraw</REQUIRES>
      <REQUIRES>withdraw_example_3_4</REQUIRES>
      <EXPECTED>50</EXPECTED>
      <SCHEME>
(withdraw 25)
      </SCHEME>
      <JAVASCRIPT>
withdraw(25);
      </JAVASCRIPT>
      <SCHEMEOUTPUT>
50
      </SCHEMEOUTPUT>
      <JAVASCRIPT_OUTPUT>
50
      </JAVASCRIPT_OUTPUT>
    </SNIPPET>
    Here successive evaluations of the same expression yield different
    values.  This behavior arises from the fact that the execution of
    assignments (in this case, assignments to the variable
    <SCHEMEINLINE>balance</SCHEMEINLINE>) delineates <EM>moments in time</EM>
    when values change.  The result of evaluating an expression depends not
    only on the expression itself, but also on whether the evaluation occurs
    before or after these moments.  Building models in terms of computational
    objects with local state forces us to confront time as an essential concept
    in programming.
  </TEXT>

  <TEXT>
    We can go further in structuring computational models to match our
    perception of the physical world.  Objects in the world do not change
    one at a time in sequence.  Rather we perceive them as acting
    <EM>concurrently</EM><EMDASH/>all at once.  So it is often natural to
    model systems as collections of computational 
    <SPLITINLINE>
      <SCHEME>processes</SCHEME>
      <JAVASCRIPT>threads</JAVASCRIPT>
    </SPLITINLINE> 
    that execute concurrently. Just as we can make our programs modular by
    organizing models in terms of objects with separate local state, it is
    often appropriate to divide computational models into parts that evolve
    separately and concurrently.  Even if the programs are to be executed on
    a sequential computer, the practice of writing programs as if they were
    to be executed concurrently forces the programmer to avoid inessential
    timing constraints and thus makes programs more modular.
  </TEXT>

  <TEXT>
    In addition to making programs more modular, concurrent computation
    can provide a speed advantage over sequential computation.  Sequential
    computers execute only one operation at a time, so the amount of time
    it takes to perform a task is proportional to the total number of
    operations performed.<FOOTNOTE>Most real processors actually execute a few
    operations at a time, following a strategy called 
    <INDEX>pipelining</INDEX>
    <EM>pipelining</EM>.  Although this technique greatly improves the
    effective
    utilization of the hardware, it is used only to speed up the execution
    of a sequential instruction stream, while retaining the behavior of
    the sequential program.</FOOTNOTE>
    However, if it is possible to decompose a problem into pieces that are
    relatively independent and need to communicate only rarely, it may be
    possible to allocate pieces to separate computing processors,
    producing a speed advantage proportional to the number of processors
    available.
  </TEXT>

  <TEXT>
    Unfortunately, the complexities introduced by assignment become even
    more problematic in the presence of concurrency.  The fact of
    concurrent execution, either because the world operates in parallel or
    because our computers do, entails additional complexity in our
    understanding of time.
  </TEXT>

  <!-- Subsection 1 : The Nature of Time in Concurrent Systems -->
  &amp;subsection3.4.1;

  <!-- Subsection 2 : Mechanisms for Controlling Concurrency -->
  &amp;subsection3.4.2;

</SECTION>
